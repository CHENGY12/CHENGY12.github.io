<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <link href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900" rel="stylesheet" />
    <script src="https://kit.fontawesome.com/38a0ffb475.js" crossorigin="anonymous"></script>
    <title>Guangyi Chen's Homepage</title>
    <link rel="icon" type="image/png" href="assets/images/icon.png">
    <!-- <script src="https://kit.fontawesome.com/38a0ffb475.js" crossorigin="anonymous"></script>
Reflux Template
https://templatemo.com/tm-531-reflux
-->
    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" />
    <!-- Additional CSS Files -->
    <link rel="stylesheet" href="assets/css/fontawesome.css" />
    <link rel="stylesheet" href="assets/css/templatemo-style.css" />
    <link rel="stylesheet" href="assets/css/owl.css" />
    <link rel="stylesheet" href="assets/css/lightbox.css" />
    <link rel="stylesheet" href="assets/css/academicons.min.css" />
</head>

<body>
    <div id="page-wraper">
        <!-- Sidebar Menu -->
        <div class="responsive-nav">
            <i class="fa fa-bars" id="menu-toggle"></i>
            <div id="menu" class="menu">
                <i class="fa fa-times" id="menu-close"></i>
                <div class="container">
                    <div class="image">
                        <a href="#"><img src="assets/images/Guangyi_Chen2.jpg" alt="" /></a>
                    </div>
                    <div class="author-content">
                        <h4>Guangyi Chen</h4>
                        <p class="author__bio">Postdoctoral Research Fellow, <br> MBZUAI & CMU </p>
                        <!--              <span>Postdoctoral Research Fellow</span>
              <span>MBZUAI, CMU</span> -->
                    </div>
                    <nav class="main-nav" role="navigation">
                        <ul class="main-menu">
                            <li><a href="#section1">About Me</a></li>
                            <li><a href="#section2">My Research</a></li>
                            <li><a href="#section3">Publications</a></li>
                            <li><a href="#section4">Teaching</a></li>
                            <li><a href="#section5">Academic Activities</a></li>
                        </ul>
                    </nav>
                    <div class="social-network">
                        <ul class="soial-icons">
                            <li>
                                <a href="mailto:guangyichen1994@gmail.com"><i class="fa fa-envelope"></i></a>
                            </li>
                            <li>
                                <a href="files/Resume_GuangyiChen.pdf"><i class="ai ai-cv"></i></a>
                            </li>
                            <li>
                                <a href="https://github.com/CHENGY12"><i class="fa-brands fa-github"></i></a>
                            </li>
                            <li>
                                <a href="https://scholar.google.com/citations?hl=zh-CN&user=3lr1jTgAAAAJ"><i class="ai ai-google-scholar-square"></i></a>
                            </li>
                            <!--       <li>
                  <a href="#"><i class="ai ai-google-scholar-square ai-2x"></i></a>
                </li> -->
                        </ul>
                    </div>
                    <div class="copyright-text">
                        <p>Copyright 2019 Reflux Design</p>
                    </div>
                </div>
            </div>
        </div>
        <section class="section about-me" data-section="section1">
            <div class="container">
                <div class="section-heading">
                    <h2>About Me</h2>
                    <div class="line-dec"></div>
                    <div class="left">
                        <p>
                            I am currently a Postdoctoral Research Fellow in MBZUAI and CMU, working with Prof. <a href="https://www.andrew.cmu.edu/user/kunz1/"> Kun Zhang </a>. I am broadly interested in computer vision and machine learning. My current research focuses on representation learning, video understaning, and causal reasoning.
                        </p>
                        <p>
                            Prior to that, I received my Ph.D degree at Tsinghua University in 2021, advised by Prof. <a href="http://www.au.tsinghua.edu.cn/info/1110/1583.htm"> Jie Zhou </a> and Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>; and obtained B.Eng. at Tsinghua University in 2016.
                        </p>
                        <p>
                            Feel free to drop me an e-mail <a href="mailto:guangyichen1994@gmail.com"> guangyichen1994(at)gmail.com </a>, if you are interested in our research and want to discuss relevant research topics or potential collaborations.
                        </p>
                    </div>
                </div>
                <div class="section-news">
                    <h2>News</h2>
                    <div class="line-dec"></div>
                    <div class="news">
                        <p>
                            <li style="margin: 5px;">
                                <b>2023-04:</b> 1 paper on graph representation learning is accepted by <a href="https://icml.cc/">ICML'2023</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2023-03:</b> Our paper using a causal perspective to understand MAE is selected as <b>Highlight (top 2.5%)</b> at <a href="https://cvpr2023.thecvf.com/">CVPR'2023</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2023-02:</b> 3 papers on causality, trajectory prediction, and prompt learning are accepted by <a href="https://cvpr2023.thecvf.com/">CVPR'2023</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2023-01:</b> 2 papers on prompt learning (<a href="https://arxiv.org/abs/2210.01253">PLOT</a> as <strong>Spotlight</strong>) and video understanding are accepted by <a href="https://iclr.cc/">ICLR'2023</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2022-11:</b> 1 paper on domain adaptation is accepted by <a href="https://aaai.org/Conferences/AAAI-23/">AAAI'2023</a> as <strong>Oral</strong>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2022-09:</b> I will serve as a publicity chair for <a href="https://www.cclear.cc/">CLeaR'2023</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2022-09:</b> 1 paper on causal representation learning is accepted by <a href="https://neurips.cc/">NeurIPS'2022</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2022-05:</b> 1 paper on domain adaptation is accepted by <a href="https://icml.cc/">ICML'2022</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2022-04:</b> 2 paper on video understanding are accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2022-03:</b> 3 paper (including 1 <strong>oral</strong>) are accepted by <a href="https://cvpr2022.thecvf.com/">CVPR'2022</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-11:</b> Our python package for causal discovery <a href="https://causal-learn.readthedocs.io/en/latest/">causal-learn</a> is released. Any feedback is welcome.
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-11:</b> I give a talk at <a href="https://www.epfl.ch/en/">EPFL</a> to introduce using causal inference for computer vision task <a href="https://chengy12.github.io/files/Talk of Causility in CV.pdf">[slides]</a>, thanks <a href="https://sites.google.com/view/yuejiangliu/home">Yuejiang</a> for the invitation!
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-07:</b> 1 paper on person re-identification and attention learning is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-07:</b> 3 paper on trajectory prediction and attention learning are accepted by <a href="http://iccv2021.thecvf.com/">ICCV'2021</a>.
                            </li>
                            <!--                             <li style="margin: 5px;">
                                <b>2021-03:</b> 1 paper on unintentional action localization is accepted by <a href="https://2021.ieeeicme.org/">ICME'2021</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2020-07:</b> 2 papers on person re-identification are accepted by <a href="https://eccv2020.eu/">ECCV'2020</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2020-06:</b> Our team pangpang (I and <a href="https://raoyongming.github.io/">Yongming</a>) won the 2nd place in Semi-Supervised Recognition Challenge at <a href="https://sites.google.com/view/fgvc7">FGVC7</a> (CVPR 2020).
                            </li>
                            <li style="margin: 5px;">
                                <b>2020-05:</b> 1 paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2019-07:</b> 2 papers are accepted by <a href="http://iccv2019.thecvf.com/">ICCV'2019</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2019-03:</b> 1 paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li> -->
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!--         <section class="section my-services" data-section="section3">
        <div class="container">
          <div class="section-heading">
            <h2>What Iâ€™m good at?</h2>
            <div class="line-dec"></div>
            <span
              >Curabitur leo felis, rutrum vitae varius eu, malesuada a tortor.
              Vestibulum congue leo et tellus aliquam, eu viverra nulla semper.
              Nullam eu faucibus diam. Donec eget massa ante.</span
            >
          </div>
          <div class="row">
            <div class="col-md-6">
              <div class="service-item">
                <div class="first-service-icon service-icon"></div>
                <h4>HTML5 &amp; CSS3</h4>
                <p>
                  Phasellus non convallis dolor. Integer tempor hendrerit arcu
                  at bibendum. Sed ac ante non metus vehicula congue quis eget
                  eros.
                </p>
              </div>
            </div>
            <div class="col-md-6">
              <div class="service-item">
                <div class="second-service-icon service-icon"></div>
                <h4>Creative Ideas</h4>
                <p>
                  Proin lacus massa, eleifend sed fermentum in, dignissim vel
                  metus. Nunc accumsan leo nec felis porttitor, ultricies
                  faucibus purus mollis.
                </p>
              </div>
            </div>
            <div class="col-md-6">
              <div class="service-item">
                <div class="third-service-icon service-icon"></div>
                <h4>Easy Customize</h4>
                <p>
                  Integer suscipit condimentum aliquet. Nam quis risus metus.
                  Nullam faucibus quam eget arcu pretium tincidunt. Nam libero
                  dui.
                </p>
              </div>
            </div>
            <div class="col-md-6">
              <div class="service-item">
                <div class="fourth-service-icon service-icon"></div>
                <h4>Admin Dashboard</h4>
                <p>
                  Vivamus et dui a massa venenatis fringilla. Proin lacus massa,
                  eleifend sed fermentum in, dignissim vel metus. Nunc accumsan
                  leo nec felis porttitor.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
       -->
        <section class="section my-research" data-section="section2">
            <div class="container">
                <div class="section-heading">
                    <h2>Research Topics</h2>
                    <div class="line-dec"></div>
                    <!-- <div class="left">
                        <p>
                            My research interests spans across Computer Vision and Machine Learning, particularly representation learning, video understaning, and causality.
                        </p>      
                    </div> -->
                    <!-- representation learning, video understaning, and causal reasoning -->
                    <div class="row">
                        <div class="isotope-wrapper">
                            <form class="isotope-toolbar">
                                <label><input type="radio" data-type="*" name="isotope-filter" />
                                    <span>All</span></label>
                                <label><input type="radio" data-type="causality" name="isotope-filter" />
                                    <span>Causality</span></label>
                                <label><input type="radio" data-type="representation" checked="" name="isotope-filter" />
                                    <span>Representation Learning</span></label>
<!--                                 <label><input type="radio" data-type="trajectory" name="isotope-filter" />
                                    <span>Trajectory Prediction</span></label> -->
                                <label><input type="radio" data-type="action" checked="" name="isotope-filter" />
                                    <span>Video Understanding</span></label>

                       <!--          <label><input type="radio" data-type="adaptation" name="isotope-filter" />
                                    <span>Transfer Learning</span></label> -->
                                <!-- <label><input type="radio" data-type="*" name="isotope-filter" />
                                    <span>All</span></label> -->
                            </form>
                            <div class="isotope-box">
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321" ondblclick="window.open('https://chengy12.github.io/files/Bosampler.pdf', '_blank');">
                                        <img src="assets/images/bosampler.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction</h4>
                                            <span>We propose to formualte the sampling process with Baysesian optimization to promote stochasitic human trajectory prediction in an unsupervised manner.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="causality">
                                    <figure class="snip1321" ondblclick="window.open('https://openreview.net/pdf?id=xnHML8u6GYk', '_blank');">
                                        <img src="assets/images/causal_mae.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Understanding Masked Autoencoders via Hierarchical Latent Variable Models</h4>
                                            <span>We propose a causal perspective to understand the underlying mechanism of MAE to identify latent variables in the hierarchical model.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321" ondblclick="window.open('https://openreview.net/pdf?id=RlPmWBiyp6w', '_blank');">
                                        <img src="assets/images/gain.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>GAIN: On the Generalization of Instructional Action Understanding</h4>
                                            <span>We present a benchmark, named GAIN, to analyze the generalizability of instructional action understanding models.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321" ondblclick="window.open('https://arxiv.org/pdf/2203.13777.pdf', '_blank');">
                                        <img src="assets/images/mid.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion</h4>
                                            <span>We propose a new framework to formulate the trajectory prediction task as a reverse process of motion indeterminacy diffusion.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321" ondblclick="window.open('https://chengy12.github.io/files/TIP-19329-2018.R2.pdf', '_blank');">
                                        <img src="assets/images/tip19.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Spatial-Temporal Attention-aware Learning for Video-based Person Re-identification</h4>
                                            <span>We propose a spatial-temporal attention to jointly discover the salient clues in both spatial and temporal domain.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321" ondblclick="window.open('https://chengy12.github.io/files/Probabilistic_Temporal_Modeling_for_Unintentional_Action_Localization.pdf', '_blank');">
                                        <img src="assets/images/tip2022a.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Probabilistic Temporal Modeling for Unintentional Action Localization</h4>
                                            <span>We propose a probabilistic framework with dense predictions to allow the uncertainty of annotations for unintentional action localization.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321" ondblclick="window.open('https://chengy12.github.io/files/Unintentional_Action_Localization_via_Counterfactual_Examples.pdf', '_blank');">
                                        <img src="assets/images/tip2022_b.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Unintentional Action Localization via Counterfactual Examples</h4>
                                            <span>We propose to disentangle the effects of content and intention clues by building a counterfactual video pool and learning hidden causal processes contrastively.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321" ondblclick="window.open('https://sites.google.com/view/finediving', '_blank');">
                                        <img src="assets/images/finediving.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment</h4>
                                            <span>We construct a new fine-grained dataset for the explainable action quality assessment, named FineDiving.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="causality">
                                    <figure class="snip1321" ondblclick="window.open('https://arxiv.org/pdf/2107.14202.pdf', '_blank');">
                                        <img src="assets/images/causal.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Human Trajectory Prediction via Counterfactual Analysis</h4>
                                            <span>We propose a counterfactual analysis method for human trajectory prediction to investigate the causality between the predictions and inputs, and alleviate the negative effects brought by environment bias.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321" ondblclick="window.open('https://arxiv.org/pdf/2107.14204.pdf', '_blank');">
                                        <img src="assets/images/model_dis.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Personalized Trajectory Prediction via Distribution Discrimination</h4>
                                            <span> We present a distribution discrimination (DisDis) method to predict personalized motion patterns by distinguishing the potential distributions in a self-supervised manner.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="causality">
                                    <figure class="snip1321" ondblclick="window.open('https://arxiv.org/abs/2108.08728', '_blank');">
                                        <img src="assets/images/CAL.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification</h4>
                                            <span>We propose to learn the attention with counterfactual causality, which provides a tool to measure the attention quality and a powerful supervisory signal to guide the learning process.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="representation">
                                    <figure class="snip1321" ondblclick="window.open('https://chengy12.github.io/files/TIP-24326-2021R2.pdf', '_blank');">
                                        <img src="assets/images/apnet.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Person Re-identification via Attention Pyramid</h4>
                                            <span>We propose attention pyramid networks by the "split-attend-merge-stack" principle to jointly learn the attentions under different scales.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321" ondblclick="window.open('https://chengy12.github.io/files/0648.pdf', '_blank');">
                                        <img src="assets/images/0648.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?</h4>
                                            <span>We show temporal coherence plays a more critical role than temporal motion for video-based person ReID and develop an adversarial feature augmentation to highlight temporal coherence.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="representation">
                                    <figure class="snip1321" ondblclick="window.open('https://chengy12.github.io/files/0645.pdf', '_blank');">
                                        <img src="assets/images/framework_0645.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Deep Credible Metric Learning for Unsupervised Domain Adaptation Person Re-identification</h4>
                                            <span>We propose to adaptively and progressively mine credible training samples to avoid the damage from the noise of predicted pseudo labels for unsupervised domain adaptation person ReID.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <!-- <div class="isotope-item" data-type="action">
                                    <figure class="snip1321">
                                        <img src="assets/images/icme21.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Temporal Label Aggregation for Unintentional Action Localization</h4>
                                            <span>We formulate the unintentional action localization as a temporal probabilistic regression problem, and propose to online aggregate multiple annotations using an attention model.</span>
                                        </figcaption>
                                    </figure>
                                </div> -->
                                <div class="isotope-item" data-type="representation">
                                    <figure class="snip1321" ondblclick="window.open('https://chengy12.github.io/files/1046_camera_ready_final.pdf', '_blank');">
                                        <img src="assets/images/1046.PNG" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Self-Critical Attention Learning for Person Re-Identification</h4>
                                            <span>We present a self-critical attention learning method which applies a critic module to examine and supervise the attention model.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="representation">
                                    <figure class="snip1321" ondblclick="window.open('https://arxiv.org/pdf/2210.01253.pdf', '_blank');">
                                        <img src="assets/images/PLOT.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Prompt Learning with Optimal Transport for Vision-Language Models</h4>
                                            <span>we propose to learn multiple comprehensive prompts with optimal transport to adapt the pretrained vision-laguage model.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="representation">
                                    <figure class="snip1321" ondblclick="window.open('https://arxiv.org/pdf/2301.04265.pdf', '_blank');">
                                        <img src="assets/images/advalign.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Adversarial Alignment for Source Free Object Detection</h4>
                                            <span>Adversarial Alignment proposes a variance-based criterion of detection to build pseudo supervisions for source free adaptative object detection.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="representation">
                                    <figure class="snip1321" ondblclick="window.open('https://denseclip.ivg-research.xyz', '_blank');">
                                        <img src="assets/images/denseclip.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</h4>
                                            <span>DenseCLIP is a new framework for dense prediction by implicitly and explicitly adapting the pre-trained knowledge from CLIP.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="causality">
                                    <figure class="snip1321" ondblclick="window.open('https://arxiv.org/pdf/2210.13647.pdf', '_blank');">
                                        <img src="assets/images/tdrl3.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4> Temporally Disentangled Representation Learning</h4>
                                            <span>A framework to recover time-delayed latent causal variables and identify their relations from sequential data under stationary environments or different distribution shifts.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="causality">
                                    <figure class="snip1321" ondblclick="window.open('https://proceedings.mlr.press/v162/kong22a/kong22a.pdf', '_blank');">
                                        <img src="assets/images/partial3.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4> Partial Disentanglement for Domain Adaptation</h4>
                                            <span>A framework with partial identifiablity to minimize unnecessary influences of domain shift with minimal changes of causal mechanisms.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
        </section>
        <section class="section publications" data-section="section3">
            <div class="container">
                <div class="section-heading">
                    <h2>Publications</h2>
                    <div class="line-dec"></div>
                    <div class="left">
                        <p>
                            Some selected recent publications. Please see <a href="https://scholar.google.com/citations?hl=zh-CN&user=3lr1jTgAAAAJ"> Google Scholar</a> <!-- <a href="https://www.andrew.cmu.edu/user/kunz1/"> Google Scholar</a> --> for details.
                        </p>
                    </div>
                    <div class="left">
                        <p>
                            <ul id="list">
                                <li style="margin: 5px;">
                                    Jiaqi Sun, Lin Zhang, <b>Guangyi Chen</b>, Kun Zhang, Peng XU, Yujiu Yang,
                                    <a href="https://arxiv.org/abs/2305.06142">Feature Expansion for Graph Neural Networks</a>,
                                    <em>International Conference on Machine Learning  (<strong>ICML</strong>)</em>, 2023.
                                    <a href="https://github.com/sajqavril/Feature-Extension-Graph-Neural-Networks">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    <b>Guangyi Chen*</b>, Zhenhao Chen*, Shunxing Fan, Kun Zhang,
                                    <a href="https://chengy12.github.io/files/Bosampler.pdf">Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction</a>,
                                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023.
                                    <a href="https://github.com/viewsetting/Unsupervised_sampling_promoting">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Lingjing Kong, Martin Q. Ma, <b>Guangyi Chen</b>, Eric P. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang,
                                    <a href="https://chengy12.github.io/files/mae_cvpr.pdf">Understanding Masked Autoencoders via Hierarchical Latent Variable Models</a>,
                                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023, <strong style="color: #8c363e;">Highlight</strong>.
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Sheng Zhang, Salman Khan, Zhiqiang Shen, Muzammal Naseer, Guangyi Chen, Fahad Khan, 
                                    <a href="https://arxiv.org/pdf/2212.05590.pdf">PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized Novel Category Discovery</a>,
                                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023.
                                    <a href="https://github.com/sheng-eatamath/PromptCAL">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    <b>Guangyi Chen</b>, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, Kun Zhang,
                                    <a href="https://arxiv.org/pdf/2210.01253.pdf">PLOT: Prompt Learning with Optimal Transport for Vision-Language Models</a>,
                                    <em>The Eleventh International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023, <strong style="color: #8c363e;">Spotlight</strong>.
                                    <a href="https://github.com/CHENGY12/PLOT/">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Junlong Li*, <b>Guangyi Chen*</b>, Yansong Tang, Jinan Bao, Kun Zhang, Jie Zhou, Jiwen Lu,
                                    <a href="https://openreview.net/forum?id=RlPmWBiyp6w">GAIN: On the Generalization of Instructional Action Understanding</a>,
                                    <em>The Eleventh International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023.
                                    [Dataset][Code]
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Qiaosong Chu, Shuyan Li, <b>Guangyi Chen</b>, Kai Li, Xiu Li,
                                    <a href="https://arxiv.org/pdf/2301.04265.pdf">Adversarial Alignment for Source Free Object Detection</a>,
                                    <em>Thirty-Seventh AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2023, <strong style="color: #8c363e;">Oral</strong>.
                                    <a href="https://github.com/ChuQiaosong/AASFOD">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Weiran Yao, <strong>Guangyi Chen</strong>, Kun Zhang,
                                    <a href="https://arxiv.org/pdf/2210.13647.pdf">Temporally Disentangled Representation Learning</a>,
                                    <em>Thirty-sixth Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022.
                                    <a href="https://www.catalyzex.com/paper/arxiv:2210.13647/code">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Lingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng, <strong>Guangyi Chen</strong>, Petar Stojanov, Victor Akinwande, Kun Zhang,
                                    <a href="https://proceedings.mlr.press/v162/kong22a/kong22a.pdf">Partial disentanglement for domain adaptation</a>,
                                    <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, 2022.
                                    <a href="https://media.icml.cc/Conferences/ICML2022/supplementary/kong22a-supp.zip">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Tianpei Gu*, <strong>Guangyi Chen*</strong>, Junlong Li, Chunze Lin, Yongming Rao, Jie Zhou, and Jiwen Lu,
                                    <a href="https://arxiv.org/pdf/2203.13777.pdf"> Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion</a>,
                                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022.
                                    <a href="https://github.com/gutianpei/MID">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Yongming Rao, Wenliang Zhao, <strong>Guangyi Chen</strong>, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu,
                                    <a href="https://arxiv.org/pdf/2112.01518.pdf">DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</a>,
                                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022.
                                    <a href="https://github.com/raoyongming/DenseCLIP">[Code]</a> <a href="https://denseclip.ivg-research.xyz">[Project]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Jinglin Xu*, Yongming Rao*, Xumin Yu, <strong>Guangyi Chen</strong>, Jie Zhou, and Jiwen Lu,
                                    <a href="https://chengy12.github.io/files/02729.pdf">FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment</a>,
                                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022, <strong style="color: #8c363e;">Oral</strong>.
                                    <a href="https://github.com/xujinglin/FineDiving">[Code]</a> <a href="https://sites.google.com/view/finediving">[Project]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Jinglin Xu*, <strong>Guangyi Chen*</strong>, Jiwen Lu, and Jie Zhou,
                                    <a href="https://chengy12.github.io/files/Unintentional_Action_Localization_via_Counterfactual_Examples.pdf">Unintentional Action Localization via Counterfactual Examples</a>,
                                    <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2022
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Jinglin Xu*, <strong>Guangyi Chen*</strong>, Jiwen Lu, and Jie Zhou,
                                    <a href="https://chengy12.github.io/files/Probabilistic_Temporal_Modeling_for_Unintentional_Action_Localization.pdf">Probabilistic Temporal Modeling for Unintentional Action Localization</a>,
                                    <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2022
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    <strong>Guangyi Chen</strong>, Junlong Li, Jiwen Lu, and Jie Zhou,
                                    <a href="https://arxiv.org/pdf/2107.14202.pdf">Human Trajectory Prediction via Counterfactual Analysis</a>,
                                    <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                                    <a href="https://github.com/CHENGY12/CausalHTP">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    <strong>Guangyi Chen</strong>, Junlong Li, Nuoxing Zhou, Liangliang Ren, and Jiwen Lu,
                                    <a href="https://arxiv.org/pdf/2107.14204.pdf">Personalized Trajectory Prediction via Distribution Discrimination</a>,
                                    <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                                    <a href="https://github.com/CHENGY12/DisDis">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    Yongming Rao*, <strong>Guangyi Chen*</strong>, Jiwen Lu and Jie Zhou,
                                    <a href="https://arxiv.org/abs/2108.08728">Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification</a>,
                                    <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                                    <a href="https://github.com/raoyongming/CAL">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    <strong>Guangyi Chen</strong>, Tianpei Gu, Jiwen Lu, Jin-An Bao, and Jie Zhou,
                                    <a href="https://chengy12.github.io/files/TIP-24326-2021R2.pdf">Person Re-identification via Attention Pyramid</a>,
                                    <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2021
                                    <a href="https://chengy12.github.io/files/TIP-24326-2021supp.pdf">[Supp]</a> <a href="https://github.com/CHENGY12/APNet">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    <strong>Guangyi Chen*</strong>, Yongming Rao*, Jiwen Lu and Jie Zhou,
                                    <a href="https://chengy12.github.io/files/0648.pdf">Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?</a>,
                                    <em>Proceedings of the European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    <strong>Guangyi Chen</strong>, Yuhao Lu, Jiwen Lu and Jie Zhou,
                                    <a href="https://chengy12.github.io/files/0645.pdf">Deep Credible Metric Learning for Unsupervised Domain Adaptation Person Re-identification</a>,
                                    <em>Proceedings of the European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    <strong>Guangyi Chen</strong>, Jiwen Lu, Ming Yang, and Jie Zhou,
                                    <a href="https://chengy12.github.io/files/TIP-21136-2019R1.pdf">Learning Recurrent 3D Attention for Video-Based Person Re-identification</a>,
                                    <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2020
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    <strong>Guangyi Chen</strong>, Tianren Zhang, Jiwen Lu and Jie Zhou,
                                    <a href="https://chengy12.github.io/files/1040_camera_ready_final.pdf">Deep Meta Metric Learning</a>,
                                    <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019
                                    <a href="https://github.com/CHENGY12/DMML">[Code]</a>
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    <strong>Guangyi Chen</strong>, Chunze Lin, Liangliang Ren, Jiwen Lu and Jie Zhou
                                    <a href="https://chengy12.github.io/files/1046_camera_ready_final.pdf">Self-Critical Attention Learning for Person Re-Identification</a>,
                                    <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019
                                    <p></p>
                                </li>
                                <li style="margin: 5px;">
                                    <strong>Guangyi Chen</strong>, Jiwen Lu, Ming Yang, and Jie Zhou,
                                    <a href="https://chengy12.github.io/files/TIP-19329-2018.R2.pdf">Spatial-Temporal Attention-aware Learning for Video-based Person Re-identification</a>,
                                    <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2019
                                    <p></p>
                                </li>
                            </ul>
                        </p>
                        <button id="toggle-button" style="float: right;">Show More</button>
                    </div>
                    <script>
                    const list = document.getElementById("list");
                    let items = list.getElementsByTagName("li");
                    if (items.length > 12) {
                        for (let i = 12; i < items.length; i++) {
                            items[i].style.display = "none"
                        }
                    }
                    const toggleButton = document.getElementById("toggle-button");
                    toggleButton.addEventListener("click", function() {
                        if (items.length > 12) {
                            for (let i = 12; i < items.length; i++) {
                                items[i].style.display = items[i].style.display === "none" ? "block" : "none";
                            }
                            toggleButton.textContent = items[12].style.display === "none" ? "Show More" : "Show Less";
                        }
                    });
                    </script>
                </div>
            </div>
        </section>
        <!--         <section class="section publications" data-section="section3">
            <div class="container">
                <div class="section-heading">
                    <h2>Publications</h2>
                    <div class="line-dec"></div>
                    <div class="left">
                        <p>
                            Some selected recent publications. Please see <a href="https://scholar.google.com/citations?hl=zh-CN&user=3lr1jTgAAAAJ"> Google Scholar</a>   for details.
                        </p>
                    </div>
                   
                    <div class="left">
                        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                            <tbody>
                                 <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/tdrl3.jpg" alt="dise"> 
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Temporally Disentangled Representation Learning</papertitle>
                                        <br>
                                        Weiran Yao, <strong>Guangyi Chen</strong>, Kun Zhang
                                        <br>
                                        <em>Thirty-sixth Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
                                        <br>
                                        <a href="https://arxiv.org/pdf/2210.13647.pdf">[Arxiv]</a> Code will come soon.
                                        <br>
                                        <p></p>
                                        <p>A framework to recover time-delayed latent causal variables and identify their relations from sequential data under stationary environments or different distribution shifts.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/partial3.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Partial disentanglement for domain adaptation</papertitle>
                                        <br>
                                        Lingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng, <strong>Guangyi Chen</strong>, Petar Stojanov, Victor Akinwande, Kun Zhang
                                        <br>
                                        <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, 2022
                                        <br>
                                        <a href="https://chengy12.github.io/files/partial.pdf">[PDF]</a> 
                                        <br>
                                        <p></p>
                                        <p>We build the data generating process using latent variable model with partial identifiability for domain adaptation, in which invariant and domain-dependent components are disentangled.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/mid.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion</papertitle>
                                        <br>
                                        Tianpei Gu*, <strong>Guangyi Chen*</strong>, Junlong Li, Chunze Lin, Yongming Rao, Jie Zhou, and Jiwen Lu
                                        <br>
                                        <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
                                        <br>
                                        <a href="https://arxiv.org/pdf/2203.13777.pdf">[Arxiv]</a> <a href="https://github.com/gutianpei/MID">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose a new framework to formulate the trajectory prediction task as a reverse process of motion indeterminacy diffusion, in which we progressively discard indeterminacy from all the walkable areas until reaching the desired trajectory.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/denseclip.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</papertitle>
                                        <br>
                                        Yongming Rao, Wenliang Zhao, <strong>Guangyi Chen</strong>, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu
                                        <br>
                                        <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
                                        <br>
                                        <a href="https://arxiv.org/pdf/2112.01518.pdf">[Arxiv]</a> <a href="https://github.com/raoyongming/DenseCLIP">[Code]</a> <a href="https://denseclip.ivg-research.xyz">[Project]</a>
                                        <br>
                                        <p></p>
                                        <p>DenseCLIP is a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP.</p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/finediving.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment</papertitle>
                                        <br>
                                        Jinglin Xu*, Yongming Rao*, Xumin Yu, <strong>Guangyi Chen</strong>, Jie Zhou, and Jiwen Lu
                                        <br>
                                         <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022, (<strong>Oral</strong>).
                                        <br>
                                        <a href="https://chengy12.github.io/files/02729.pdf">[PDF]</a> <a href="https://github.com/xujinglin/FineDiving">[Code]</a> <a href="https://sites.google.com/view/finediving">[Project]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose a counterfactual analysis method for human trajectory prediction to alleviate the negative effects brought by environment bias.</p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/tip2022_b.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Unintentional Action Localization via Counterfactual Examples</papertitle>
                                        <br>
                                        Jinglin Xu*, <strong>Guangyi Chen*</strong>, Jiwen Lu, and Jie Zhou
                                        <br>
                                        <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2022
                                        <br>
                                        <a href="https://chengy12.github.io/files/Unintentional_Action_Localization_via_Counterfactual_Examples.pdf">[PDF]</a> 
                                        <br>
                                        <p></p>
                                        <p>We propose an approach to disentangle the effects of content and intention clues by building a counterfactual video pool, which mitigates the negative effect brought by biased action content and highlights the causal effect of intention on model prediction.
                                        </p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/tip2022a.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Probabilistic Temporal Modeling for Unintentional Action Localization</papertitle>
                                        <br>
                                        Jinglin Xu*, <strong>Guangyi Chen*</strong>, Jiwen Lu, and Jie Zhou
                                        <br>
                                        <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2022
                                        <br>
                                        <a href="https://chengy12.github.io/files/Probabilistic_Temporal_Modeling_for_Unintentional_Action_Localization.pdf">[PDF]</a> 
                                        <br>
                                        <p></p>
                                        <p>We propose a probabilistic framework for unintentional action localization, in which we model the uncertainty of annotations with temporal label aggregation and use it for training a dense probabilistic localization model.
                                        </p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/causal.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Human Trajectory Prediction via Counterfactual Analysis</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Junlong Li, Jiwen Lu, and Jie Zhou
                                        <br>
                                        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                                        <br>
                                        <a href="https://arxiv.org/pdf/2107.14202.pdf">[Arxiv]</a> <a href="https://github.com/CHENGY12/CausalHTP">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose a counterfactual analysis method for human trajectory prediction to alleviate the negative effects brought by environment bias.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/model_dis.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Personalized Trajectory Prediction via Distribution Discrimination</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Junlong Li, Nuoxing Zhou, Liangliang Ren, and Jiwen Lu
                                        <br>
                                        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                                        <br>
                                        <a href="https://arxiv.org/pdf/2107.14204.pdf">[Arxiv]</a> <a href="https://github.com/CHENGY12/DisDis">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p> We present a distribution discrimination (DisDis) method to predict personalized motion patterns by distinguishing the potential distributions in a self-supervised manner.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/CAL.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification</papertitle>
                                        <br>
                                        Yongming Rao*, <strong>Guangyi Chen*</strong>, Jiwen Lu and Jie Zhou
                                        <br>
                                        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                                        <br>
                                        <a href="https://arxiv.org/abs/2108.08728">[Arxiv]</a> <a href="https://github.com/raoyongming/CAL">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose to learn the attention with counterfactual causality, which provides a tool to measure the attention quality and a powerful supervisory signal to guide the learning process.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/apnet.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Person Re-identification via Attention Pyramid</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Tianpei Gu, Jiwen Lu, Jin-An Bao, and Jie Zhou
                                        <br>
                                        <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2021
                                        <br>
                                        <a href="https://chengy12.github.io/files/TIP-24326-2021R2.pdf">[PDF]</a> <a href="https://chengy12.github.io/files/TIP-24326-2021supp.pdf">[Supp]</a> <a href="https://github.com/CHENGY12/APNet">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose attention pyramid networks by the "split-attend-merge-stack" principle to jointly learn the attentions under different scales and obtain superior performance on many person re-identification datasets.
                                        </p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/icme21.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Temporal Label Aggregation for Unintentional Action Localization</papertitle>
                                        <br>
                                        Nuoxing Zhou, <strong>Guangyi Chen</strong>, Jinglin Xu, Weishi Zheng, and Jiwen Lu
                                        <br>
                                        <em>2021 IEEE International Conference on Multimedia and Expo (<strong>ICME</strong>)</em>, 2021
                                        <br>
                                        <a href="https://chengy12.github.io/files/icme2021_final.pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p>We formulate the unintentional action localization as a temporal probabilistic regression problem, and propose to online aggregate multiple annotations using an attention model.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/0648.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?</papertitle>
                                        <br>
                                        <strong>Guangyi Chen*</strong>, Yongming Rao*, Jiwen Lu and Jie Zhou
                                        <br>
                                        <em>Proceedings of the European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
                                        <br>
                                        <a href="https://chengy12.github.io/files/0648.pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p>We show temporal coherence plays a more critical role than temporal motion for video-based person ReID and develop an adversarial feature augmentation to highlight temporal coherence.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/framework_0645.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Deep Credible Metric Learning for Unsupervised Domain Adaptation Person Re-identification</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Yuhao Lu, Jiwen Lu and Jie Zhou
                                        <br>
                                        <em>16th European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
                                        <br>
                                        <a href="https://chengy12.github.io/files/0645.pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose to adaptively and progressively mine credible training samples to avoid the damage from the noise of predicted pseudo labels for unsupervised domain adaptation person ReID.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/motivation_1040.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Deep Meta Metric Learning</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Tianren Zhang, Jiwen Lu and Jie Zhou
                                        <br>
                                        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019
                                        <br>
                                        <a href="https://chengy12.github.io/files/1040_camera_ready_final.pdf">[PDF]</a> <a href="https://github.com/CHENGY12/DMML">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose to understand the deep metric learning via meta-learning.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/1046.PNG" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Self-Critical Attention Learning for Person Re-Identification</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Chunze Lin, Liangliang Ren, Jiwen Lu and Jie Zhou
                                        <br>
                                        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019
                                        <br>
                                        <a href="https://chengy12.github.io/files/1046_camera_ready_final.pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p>We present a self-critical attention learning method which applies a critic module to examine and supervise the attention model.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src='assets/images/tip2020_network.jpg' alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Learning Recurrent 3D Attention for Video-Based Person Re-identification</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Jiwen Lu, Ming Yang, and Jie Zhou
                                        <br>
                                        <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2020
                                        <br>
                                        <a href="https://chengy12.github.io/files/TIP-21136-2019R1.pdf">[PDF]</a>
                                        <br>
                                        <p> We propose to recurrently discover the 3D attention regions and use the reinforcement learning for optimization. </p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src='assets/images/tip19.jpg' alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Spatial-Temporal Attention-aware Learning for Video-based Person Re-identification</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Jiwen Lu, Ming Yang, and Jie Zhou
                                        <br>
                                        <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2019
                                        <br>
                                        <a href="https://chengy12.github.io/files/TIP-19329-2018.R2.pdf">[PDF]</a>
                                        <br>
                                        <p> We propose a spatial-temporal attention to jointly discover the salient clues in both spatial and temporal domain. </p>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </section> -->
        <section class="section teaching" data-section="section4">
            <div class="container">
                <div class="section-heading">
                    <h2>Teaching</h2>
                    <div class="line-dec"></div>
                </div>
                <div class="news">
                    <p>
                        <li style="margin: 5px;">
                            <b>TA</b> Numerical Analysis and Algorithm, Tsinghua University.
                        </li>
                        <li style="margin: 5px;">
                            <b>TA</b> Analog Electronic Technology Foundation, Tsinghua University.
                        </li>
                        <li style="margin: 5px;">
                            <b>TA</b> Probabilistic and Statistical Inference (ML703), MBZUAI.
                        </li>
                    </p>
                </div>
            </div>
        </section>
        <!--         <section class="section services" data-section="section5">
            <div class="container">
                <div class="section-services">
                    <h2>Services</h2>
                    <div class="line-dec"></div>
                </div>
                <div class="news">
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Competition Awards</heading>
                                    <p>
                                        <li style="margin: 5px;"> 2nd place in Semi-Supervised Recognition Challenge at FGVC7 (CVPR 2020)</li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Academic Services</heading>
                                    <p>
                                        <li style="margin: 5px;">
                                            <b>Co-organizer:</b> for the ICME 2019 workshop: The Third Workshop on Human Identification in Multimedia (HIM'19) <a href="http://ivg.au.tsinghua.edu.cn/him19/"> [website]</a>
                                        </li>
                                        <li style="margin: 5px;">
                                            <b>Conference Reviewer / Program Committee Member:</b> CVPR, ICCV, ICML, NeurIPS, ICLR and so on.
                                        </li>
                                        <li style="margin: 5px;">
                                            <b>Journal Reviewer:</b> TIP, TMM, TCSVT and so on.
                                        </li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </section> -->
        <section class="section services" data-section="section5">
            <div class="container">
                <div class="section-heading">
                    <h2>Academic Activities</h2>
                    <div class="line-dec"></div>
                    <h3>Competitions</h3>
                    <div class="left">
                        <p>
                            <li style="margin: 5px;"> 2nd place in Semi-Supervised Recognition Challenge at FGVC7 (CVPR 2020)</li>
                        </p>
                    </div>
                    <h3>Academic Services</h3>
                    <div class="left">
                        <p>
                            <li style="margin: 5px;">
                                <b>Publicity Chair:</b> for CLeaR 2023.
                            </li>
                            <li style="margin: 5px;">
                                <b>Co-organizer:</b> for the ICME 2019 workshop: The Third Workshop on Human Identification in Multimedia (HIM'19) <a href="http://ivg.au.tsinghua.edu.cn/him19/"> [website]</a>
                            </li>
                            <li style="margin: 5px;">
                                <b>Conference Reviewer / Program Committee Member:</b> CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR and so on.
                            </li>
                            <li style="margin: 5px;">
                                <b>Journal Reviewer:</b> TPAMI, TIP, IJCV, TMM, TCSVT and so on.
                            </li>
                        </p>
                    </div>
                </div>
            </div>
        </section>
    </div>
    <!-- Scripts -->
    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/js/isotope.min.js"></script>
    <script src="assets/js/owl-carousel.js"></script>
    <script src="assets/js/lightbox.js"></script>
    <script src="assets/js/custom.js"></script>
    <script>
    //according to loftblog tut
    $(".main-menu li:first").addClass("active");

    var showSection = function showSection(section, isAnimate) {
        var direction = section.replace(/#/, ""),
            reqSection = $(".section").filter(
                '[data-section="' + direction + '"]'
            ),
            reqSectionPos = reqSection.offset().top - 0;

        if (isAnimate) {
            $("body, html").animate({
                    scrollTop: reqSectionPos
                },
                800
            );
        } else {
            $("body, html").scrollTop(reqSectionPos);
        }
    };

    var checkSection = function checkSection() {
        $(".section").each(function() {
            var $this = $(this),
                topEdge = $this.offset().top - 80,
                bottomEdge = topEdge + $this.height(),
                wScroll = $(window).scrollTop();
            if (topEdge < wScroll && bottomEdge > wScroll) {
                var currentId = $this.data("section"),
                    reqLink = $("a").filter("[href*=\\#" + currentId + "]");
                reqLink
                    .closest("li")
                    .addClass("active")
                    .siblings()
                    .removeClass("active");
            }
        });
    };

    $(".main-menu").on("click", "a", function(e) {
        e.preventDefault();
        showSection($(this).attr("href"), true);
    });

    $(window).scroll(function() {
        checkSection();
    });
    </script>
</body>

</html>