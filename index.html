<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <link href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900" rel="stylesheet" />
    <script src="https://kit.fontawesome.com/38a0ffb475.js" crossorigin="anonymous"></script>
    <title>Guangyi Chen's Homepage</title>
    <link rel="icon" type="image/png" href="assets/images/icon.png">
    <!-- <script src="https://kit.fontawesome.com/38a0ffb475.js" crossorigin="anonymous"></script>
Reflux Template
https://templatemo.com/tm-531-reflux
-->
    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" />
    <!-- Additional CSS Files -->
    <link rel="stylesheet" href="assets/css/fontawesome.css" />
    <link rel="stylesheet" href="assets/css/templatemo-style.css" />
    <link rel="stylesheet" href="assets/css/owl.css" />
    <link rel="stylesheet" href="assets/css/lightbox.css" />
    <link rel="stylesheet" href="assets/css/academicons.min.css" />
</head>

<body>
    <div id="page-wraper">
        <!-- Sidebar Menu -->
        <div class="responsive-nav">
            <i class="fa fa-bars" id="menu-toggle"></i>
            <div id="menu" class="menu">
                <i class="fa fa-times" id="menu-close"></i>
                <div class="container">
                    <div class="image">
                        <a href="#"><img src="assets/images/Guangyi_Chen2.jpg" alt="" /></a>
                    </div>
                    <div class="author-content">
                        <h4>Guangyi Chen</h4>
                        <p class="author__bio">Postdoctoral Research Fellow, <br> MBZUAI & CMU </p>
                        <!--              <span>Postdoctoral Research Fellow</span>
              <span>MBZUAI, CMU</span> -->
                    </div>
                    <nav class="main-nav" role="navigation">
                        <ul class="main-menu">
                            <li><a href="#section1">About Me</a></li>
                            <!-- <li><a href="#section2">My Research</a></li> -->
                            <li><a href="#section3">Publications</a></li>
                            <li><a href="#section4">Teaching</a></li>
                            <li><a href="#section5">Academic Activities</a></li>
                        </ul>
                    </nav>
                    <div class="social-network">
                        <ul class="soial-icons">
                            <li>
                                <a href="mailto:guangyichen1994@gmail.com"><i class="fa fa-envelope"></i></a>
                            </li>
                            <li>
                                <a href="files/Resume_GuangyiChen.pdf"><i class="ai ai-cv"></i></a>
                            </li>
                            <li>
                                <a href="https://github.com/CHENGY12"><i class="fa-brands fa-github"></i></a>
                            </li>
                            <li>
                                <a href="https://scholar.google.com/citations?hl=zh-CN&user=3lr1jTgAAAAJ"><i class="ai ai-google-scholar-square"></i></a>
                            </li>
                            <!--       <li>
                  <a href="#"><i class="ai ai-google-scholar-square ai-2x"></i></a>
                </li> -->
                        </ul>
                    </div>
                    <div class="copyright-text">
                        <p>Copyright 2019 Reflux Design</p>
                    </div>
                </div>
            </div>
        </div>
        <section class="section about-me" data-section="section1">
            <div class="container">
                <div class="section-heading">
                    <h2>About Me</h2>
                    <div class="line-dec"></div>
                    <div class="left">
                        <p>
                            I am currently a Postdoctoral Research Fellow in MBZUAI and CMU, working with Prof. <a href="https://www.andrew.cmu.edu/user/kunz1/"> Kun Zhang </a>. I am broadly interested in computer vision and machine learning. My current research focuses on causality and representation learning in ML, and video understanding, trajectory prediction, and person re-identification in CV.
                        </p>
                        <p>
                            Prior to that, I received my Ph.D degree at Tsinghua University in 2021, advised by Prof. <a href="http://www.au.tsinghua.edu.cn/info/1110/1583.htm"> Jie Zhou </a> and Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>; and obtained B.Eng. at Tsinghua University In 2016.
                        </p>
                        <p>
                            If you are interested in our work and want to join us, please do not hesitate to drop me an email, <a href="mailto:guangyichen1994@gmail.com"> guangyichen1994(at)gmail.com </a>, with your resume.
                        </p>
                    </div>
                </div>
                <div class="section-news">
                    <h2>News</h2>
                    <div class="line-dec"></div>
                    <div class="news">
                        <p> 
                        	<li style="margin: 5px;">
                                <b>2022-11:</b> 1 paper on domain adaptation is accepted by <a href="https://aaai.org/Conferences/AAAI-23/">AAAI'2023</a>.
                            </li>
                        	<li style="margin: 5px;">
                                <b>2022-09:</b> I will serve as a publicity chair for <a href="https://www.cclear.cc/">CLeaR'2023</a>.
                            </li>
                        	<li style="margin: 5px;">
                                <b>2022-09:</b> 1 paper on causal representation learning is accepted by <a href="https://neurips.cc/">NeurIPS'2022</a>.
                            </li>
                        	<li style="margin: 5px;">
                                <b>2022-05:</b> 1 paper on domain adaptation is accepted by <a href="https://icml.cc/">ICML'2022</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2022-04:</b> 1 paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2022-03:</b> 1 paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2022-03:</b> 3 paper is accepted by <a href="https://cvpr2022.thecvf.com/">CVPR'2022</a>, 1 oral.
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-11:</b> Our python package for causal discovery <a href="https://causal-learn.readthedocs.io/en/latest/">causal-learn</a> is released. Any feedback is welcome.
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-11:</b> I give a talk at <a href="https://www.epfl.ch/en/">EPFL</a> to introduce using causal inference for computer vision task <a href="https://chengy12.github.io/files/Talk of Causility in CV.pdf">[slides]</a>, thanks <a href="https://sites.google.com/view/yuejiangliu/home">Yuejiang</a> for the invitation!
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-07:</b> 1 paper on person re-identification and attention learning is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2021-07:</b> 3 paper on trajectory prediction and attention learning is accepted by <a href="http://iccv2021.thecvf.com/">ICCV'2021</a>.
                            </li>
<!--                             <li style="margin: 5px;">
                                <b>2021-03:</b> 1 paper on unintentional action localization is accepted by <a href="https://2021.ieeeicme.org/">ICME'2021</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2020-07:</b> 2 papers on person re-identification are accepted by <a href="https://eccv2020.eu/">ECCV'2020</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2020-06:</b> Our team pangpang (I and <a href="https://raoyongming.github.io/">Yongming</a>) won the 2nd place in Semi-Supervised Recognition Challenge at <a href="https://sites.google.com/view/fgvc7">FGVC7</a> (CVPR 2020).
                            </li>
                            <li style="margin: 5px;">
                                <b>2020-05:</b> 1 paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2019-07:</b> 2 papers are accepted by <a href="http://iccv2019.thecvf.com/">ICCV'2019</a>.
                            </li>
                            <li style="margin: 5px;">
                                <b>2019-03:</b> 1 paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                            </li> -->
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!--         <section class="section my-services" data-section="section3">
        <div class="container">
          <div class="section-heading">
            <h2>What I’m good at?</h2>
            <div class="line-dec"></div>
            <span
              >Curabitur leo felis, rutrum vitae varius eu, malesuada a tortor.
              Vestibulum congue leo et tellus aliquam, eu viverra nulla semper.
              Nullam eu faucibus diam. Donec eget massa ante.</span
            >
          </div>
          <div class="row">
            <div class="col-md-6">
              <div class="service-item">
                <div class="first-service-icon service-icon"></div>
                <h4>HTML5 &amp; CSS3</h4>
                <p>
                  Phasellus non convallis dolor. Integer tempor hendrerit arcu
                  at bibendum. Sed ac ante non metus vehicula congue quis eget
                  eros.
                </p>
              </div>
            </div>
            <div class="col-md-6">
              <div class="service-item">
                <div class="second-service-icon service-icon"></div>
                <h4>Creative Ideas</h4>
                <p>
                  Proin lacus massa, eleifend sed fermentum in, dignissim vel
                  metus. Nunc accumsan leo nec felis porttitor, ultricies
                  faucibus purus mollis.
                </p>
              </div>
            </div>
            <div class="col-md-6">
              <div class="service-item">
                <div class="third-service-icon service-icon"></div>
                <h4>Easy Customize</h4>
                <p>
                  Integer suscipit condimentum aliquet. Nam quis risus metus.
                  Nullam faucibus quam eget arcu pretium tincidunt. Nam libero
                  dui.
                </p>
              </div>
            </div>
            <div class="col-md-6">
              <div class="service-item">
                <div class="fourth-service-icon service-icon"></div>
                <h4>Admin Dashboard</h4>
                <p>
                  Vivamus et dui a massa venenatis fringilla. Proin lacus massa,
                  eleifend sed fermentum in, dignissim vel metus. Nunc accumsan
                  leo nec felis porttitor.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
       -->
        <!-- <section class="section my-research" data-section="section2">
            <div class="container">
                <div class="section-heading">
                    <h2>My Research</h2>
                    <div class="line-dec"></div>
                    <div class="left">
                        <p>
                            My research interests spans across Computer Vision and Machine Learning. For computer vision, I focus on developing methods to better understand human, which includes person re-identification, pedestrian trajectory prediction, and human action understanding. On the machine learning site, I am interested in causal-based learning such as causal inference and causal representation learning.
                        </p>
                    </div>
                    <div class="row">
                        <div class="isotope-wrapper">
                            <form class="isotope-toolbar">
                                <label><input type="radio" data-type="*" checked="" name="isotope-filter" />
                                    <span>All</span></label>
                                <label><input type="radio" data-type="reid" name="isotope-filter" />
                                    <span>Re-identification</span></label>
                                <label><input type="radio" data-type="trajectory" name="isotope-filter" />
                                    <span>Trajectory</span></label>
                                <label><input type="radio" data-type="action" name="isotope-filter" />
                                    <span>Action</span></label>
                                <label><input type="radio" data-type="causality" name="isotope-filter" />
                                    <span>Causality</span></label>
                            </form>
                            <div class="isotope-box">
                                <div class="isotope-item" data-type="trajectory">
                                    <figure class="snip1321">
                                        <img src="assets/images/mid.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion</h4>
                                            <span>We propose a new framework to formulate the trajectory prediction task as a reverse process of motion indeterminacy diffusion, in which we progressively discard indeterminacy from all the walkable areas until reaching the desired trajectory.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321">
                                        <img src="assets/images/finediving.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment</h4>
                                            <span>We construct a new fine-grained dataset for the explainable action quality assessment, named FineDiving, developed on diverse diving events with detailed annotations on action procedures.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="trajectory">
                                    <figure class="snip1321">
                                        <img src="assets/images/causal.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Human Trajectory Prediction via Counterfactual Analysis</h4>
                                            <span>We propose a counterfactual analysis method for human trajectory prediction to investigate the causality between the predicted trajectories and input clues and alleviate the negative effects brought by environment bias.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="trajectory">
                                    <figure class="snip1321">
                                        <img src="assets/images/model_dis.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Personalized Trajectory Prediction via Distribution Discrimination</h4>
                                            <span> We present a distribution discrimination (DisDis) method to predict personalized motion patterns by distinguishing the potential distributions in a self-supervised manner.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="causality">
                                    <figure class="snip1321">
                                        <img src="assets/images/CAL.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification</h4>
                                            <span>We propose to learn the attention with counterfactual causality, which provides a tool to measure the attention quality and a powerful supervisory signal to guide the learning process.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="reid">
                                    <figure class="snip1321">
                                        <img src="assets/images/apnet.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Person Re-identification via Attention Pyramid</h4>
                                            <span>We propose attention pyramid networks by the "split-attend-merge-stack" principle to jointly learn the attentions under different scales and obtain superior performance on many person re-identification datasets.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="reid">
                                    <figure class="snip1321">
                                        <img src="assets/images/0648.jpg" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?</h4>
                                            <span>We show temporal coherence plays a more critical role than temporal motion for video-based person ReID and develop an adversarial feature augmentation to highlight temporal coherence.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="reid">
                                    <figure class="snip1321">
                                        <img src="assets/images/framework_0645.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Deep Credible Metric Learning for Unsupervised Domain Adaptation Person Re-identification</h4>
                                            <span>We propose to adaptively and progressively mine credible training samples to avoid the damage from the noise of predicted pseudo labels for unsupervised domain adaptation person ReID.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="action">
                                    <figure class="snip1321">
                                        <img src="assets/images/icme21.png" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Temporal Label Aggregation for Unintentional Action Localization</h4>
                                            <span>We formulate the unintentional action localization as a temporal probabilistic regression problem, and propose to online aggregate multiple annotations using an attention model.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                                <div class="isotope-item" data-type="reid">
                                    <figure class="snip1321">
                                        <img src="assets/images/1046.PNG" alt="sq-sample26" />
                                        <figcaption>
                                            <h4>Self-Critical Attention Learning for Person Re-Identification</h4>
                                            <span>We present a self-critical attention learning method which applies a critic module to examine and supervise the attention model.</span>
                                        </figcaption>
                                    </figure>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
        </section> -->
        <section class="section publications" data-section="section3">
            <div class="container">
                <div class="section-heading">
                    <h2>Publications</h2>
                    <div class="line-dec"></div>
                    <!--                 </div>
                <div class="row"> -->
                    <div class="left">
                        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                            <tbody>
                            	 <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/tdrl3.jpg" alt="dise"> 
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Temporally Disentangled Representation Learning</papertitle>
                                        <br>
                                        Weiran Yao, <strong>Guangyi Chen</strong>, Kun Zhang
                                        <br>
                                        <em>Thirty-sixth Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
                                        <br>
                                        <a href="https://arxiv.org/pdf/2210.13647.pdf">[Arxiv]</a> Code will come soon.
                                        <br>
                                        <p></p>
                                        <p>A framework to recover time-delayed latent causal variables and identify their relations from sequential data under stationary environments or different distribution shifts.</p>
                                    </td>
                                </tr>
                                <!-- <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/partial3.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Partial disentanglement for domain adaptation</papertitle>
                                        <br>
                                        Lingjing Kong, Shaoan Xie, Weiran Yao, Yujia Zheng, <strong>Guangyi Chen</strong>, Petar Stojanov, Victor Akinwande, Kun Zhang
                                        <br>
                                        <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, 2022
                                        <br>
                                        <a href="https://chengy12.github.io/files/partial.pdf">[PDF]</a> 
                                        <br>
                                        <p></p>
                                        <p>We build the data generating process using latent variable model with partial identifiability for domain adaptation, in which invariant and domain-dependent components are disentangled.</p>
                                    </td>
                                </tr>-->
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/mid.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion</papertitle>
                                        <br>
                                        Tianpei Gu*, <strong>Guangyi Chen*</strong>, Junlong Li, Chunze Lin, Yongming Rao, Jie Zhou, and Jiwen Lu
                                        <br>
                                        <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
                                        <br>
                                        <a href="https://arxiv.org/pdf/2203.13777.pdf">[Arxiv]</a> <a href="https://github.com/gutianpei/MID">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose a new framework to formulate the trajectory prediction task as a reverse process of motion indeterminacy diffusion, in which we progressively discard indeterminacy from all the walkable areas until reaching the desired trajectory.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/denseclip.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</papertitle>
                                        <br>
                                        Yongming Rao, Wenliang Zhao, <strong>Guangyi Chen</strong>, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu
                                        <br>
                                        <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
                                        <br>
                                        <a href="https://arxiv.org/pdf/2112.01518.pdf">[Arxiv]</a> <a href="https://github.com/raoyongming/DenseCLIP">[Code]</a> <a href="https://denseclip.ivg-research.xyz">[Project]</a>
                                        <br>
                                        <p></p>
                                        <p>DenseCLIP is a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP.</p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/finediving.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment</papertitle>
                                        <br>
                                        Jinglin Xu*, Yongming Rao*, Xumin Yu, <strong>Guangyi Chen</strong>, Jie Zhou, and Jiwen Lu
                                        <br>
                                         <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022, (<strong>Oral</strong>).
                                        <br>
                                        <a href="https://chengy12.github.io/files/02729.pdf">[PDF]</a> <a href="https://github.com/xujinglin/FineDiving">[Code]</a> <a href="https://sites.google.com/view/finediving">[Project]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose a counterfactual analysis method for human trajectory prediction to alleviate the negative effects brought by environment bias.</p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/tip2022_b.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Unintentional Action Localization via Counterfactual Examples</papertitle>
                                        <br>
                                        Jinglin Xu*, <strong>Guangyi Chen*</strong>, Jiwen Lu, and Jie Zhou
                                        <br>
                                        <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2022
                                        <br>
                                        <a href="https://chengy12.github.io/files/Unintentional_Action_Localization_via_Counterfactual_Examples.pdf">[PDF]</a> 
                                        <br>
                                        <p></p>
                                        <p>We propose an approach to disentangle the effects of content and intention clues by building a counterfactual video pool, which mitigates the negative effect brought by biased action content and highlights the causal effect of intention on model prediction.
                                        </p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/tip2022a.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Probabilistic Temporal Modeling for Unintentional Action Localization</papertitle>
                                        <br>
                                        Jinglin Xu*, <strong>Guangyi Chen*</strong>, Jiwen Lu, and Jie Zhou
                                        <br>
                                        <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2022
                                        <br>
                                        <a href="https://chengy12.github.io/files/Probabilistic_Temporal_Modeling_for_Unintentional_Action_Localization.pdf">[PDF]</a> 
                                        <br>
                                        <p></p>
                                        <p>We propose a probabilistic framework for unintentional action localization, in which we model the uncertainty of annotations with temporal label aggregation and use it for training a dense probabilistic localization model.
                                        </p>
                                    </td>
                                </tr>

                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/causal.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Human Trajectory Prediction via Counterfactual Analysis</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Junlong Li, Jiwen Lu, and Jie Zhou
                                        <br>
                                        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                                        <br>
                                        <a href="https://arxiv.org/pdf/2107.14202.pdf">[Arxiv]</a> <a href="https://github.com/CHENGY12/CausalHTP">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose a counterfactual analysis method for human trajectory prediction to alleviate the negative effects brought by environment bias.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/model_dis.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Personalized Trajectory Prediction via Distribution Discrimination</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Junlong Li, Nuoxing Zhou, Liangliang Ren, and Jiwen Lu
                                        <br>
                                        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                                        <br>
                                        <a href="https://arxiv.org/pdf/2107.14204.pdf">[Arxiv]</a> <a href="https://github.com/CHENGY12/DisDis">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p> We present a distribution discrimination (DisDis) method to predict personalized motion patterns by distinguishing the potential distributions in a self-supervised manner.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/CAL.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification</papertitle>
                                        <br>
                                        Yongming Rao*, <strong>Guangyi Chen*</strong>, Jiwen Lu and Jie Zhou
                                        <br>
                                        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                                        <br>
                                        <a href="https://arxiv.org/abs/2108.08728">[Arxiv]</a> <a href="https://github.com/raoyongming/CAL">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose to learn the attention with counterfactual causality, which provides a tool to measure the attention quality and a powerful supervisory signal to guide the learning process.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/apnet.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Person Re-identification via Attention Pyramid</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Tianpei Gu, Jiwen Lu, Jin-An Bao, and Jie Zhou
                                        <br>
                                        <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2021
                                        <br>
                                        <a href="https://chengy12.github.io/files/TIP-24326-2021R2.pdf">[PDF]</a> <a href="https://chengy12.github.io/files/TIP-24326-2021supp.pdf">[Supp]</a> <a href="https://github.com/CHENGY12/APNet">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose attention pyramid networks by the "split-attend-merge-stack" principle to jointly learn the attentions under different scales and obtain superior performance on many person re-identification datasets.
                                        </p>
                                    </td>
                                </tr>
                                <!-- <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/icme21.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Temporal Label Aggregation for Unintentional Action Localization</papertitle>
                                        <br>
                                        Nuoxing Zhou, <strong>Guangyi Chen</strong>, Jinglin Xu, Weishi Zheng, and Jiwen Lu
                                        <br>
                                        <em>2021 IEEE International Conference on Multimedia and Expo (<strong>ICME</strong>)</em>, 2021
                                        <br>
                                        <a href="https://chengy12.github.io/files/icme2021_final.pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p>We formulate the unintentional action localization as a temporal probabilistic regression problem, and propose to online aggregate multiple annotations using an attention model.</p>
                                    </td>
                                </tr> -->
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/0648.jpg" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?</papertitle>
                                        <br>
                                        <strong>Guangyi Chen*</strong>, Yongming Rao*, Jiwen Lu and Jie Zhou
                                        <br>
                                        <em>Proceedings of the European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
                                        <br>
                                        <a href="https://chengy12.github.io/files/0648.pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p>We show temporal coherence plays a more critical role than temporal motion for video-based person ReID and develop an adversarial feature augmentation to highlight temporal coherence.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/framework_0645.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Deep Credible Metric Learning for Unsupervised Domain Adaptation Person Re-identification</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Yuhao Lu, Jiwen Lu and Jie Zhou
                                        <br>
                                        <em>16th European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
                                        <br>
                                        <a href="https://chengy12.github.io/files/0645.pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose to adaptively and progressively mine credible training samples to avoid the damage from the noise of predicted pseudo labels for unsupervised domain adaptation person ReID.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/motivation_1040.png" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Deep Meta Metric Learning</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Tianren Zhang, Jiwen Lu and Jie Zhou
                                        <br>
                                        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019
                                        <br>
                                        <a href="https://chengy12.github.io/files/1040_camera_ready_final.pdf">[PDF]</a> <a href="https://github.com/CHENGY12/DMML">[Code]</a>
                                        <br>
                                        <p></p>
                                        <p>We propose to understand the deep metric learning via meta-learning.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src="assets/images/1046.PNG" alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Self-Critical Attention Learning for Person Re-Identification</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Chunze Lin, Liangliang Ren, Jiwen Lu and Jie Zhou
                                        <br>
                                        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019
                                        <br>
                                        <a href="https://chengy12.github.io/files/1046_camera_ready_final.pdf">[PDF]</a>
                                        <br>
                                        <p></p>
                                        <p>We present a self-critical attention learning method which applies a critic module to examine and supervise the attention model.</p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src='assets/images/tip2020_network.jpg' alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Learning Recurrent 3D Attention for Video-Based Person Re-identification</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Jiwen Lu, Ming Yang, and Jie Zhou
                                        <br>
                                        <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2020
                                        <br>
                                        <a href="https://chengy12.github.io/files/TIP-21136-2019R1.pdf">[PDF]</a>
                                        <br>
                                        <p> We propose to recurrently discover the 3D attention regions and use the reinforcement learning for optimization. </p>
                                    </td>
                                </tr>
                                <tr>
                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                        <img style="width:100%;max-width:100%" src='assets/images/tip19.jpg' alt="dise">
                                    </td>
                                    <td width="75%" valign="center">
                                        <papertitle>Spatial-Temporal Attention-aware Learning for Video-based Person Re-identification</papertitle>
                                        <br>
                                        <strong>Guangyi Chen</strong>, Jiwen Lu, Ming Yang, and Jie Zhou
                                        <br>
                                        <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2019
                                        <br>
                                        <a href="https://chengy12.github.io/files/TIP-19329-2018.R2.pdf">[PDF]</a>
                                        <br>
                                        <p> We propose a spatial-temporal attention to jointly discover the salient clues in both spatial and temporal domain. </p>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </section>
        <section class="section teaching" data-section="section4">
            <div class="container">
                <div class="section-heading">
                    <h2>Teaching</h2>
                    <div class="line-dec"></div>
                </div>
                <div class="news">
                    <p>
                        <li style="margin: 5px;">
                            <b>TA</b> Numerical Analysis and Algorithm, Tsinghua University.
                        </li>
                        <li style="margin: 5px;">
                            <b>TA</b> Analog Electronic Technology Foundation, Tsinghua University.
                        </li>
                        <li style="margin: 5px;">
                            <b>TA</b> Probabilistic and Statistical Inference (ML703), MBZUAI.
                        </li>
                    </p>
                </div>
            </div>
        </section>
        <!--         <section class="section services" data-section="section5">
            <div class="container">
                <div class="section-services">
                    <h2>Services</h2>
                    <div class="line-dec"></div>
                </div>
                <div class="news">
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Competition Awards</heading>
                                    <p>
                                        <li style="margin: 5px;"> 2nd place in Semi-Supervised Recognition Challenge at FGVC7 (CVPR 2020)</li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Academic Services</heading>
                                    <p>
                                        <li style="margin: 5px;">
                                            <b>Co-organizer:</b> for the ICME 2019 workshop: The Third Workshop on Human Identification in Multimedia (HIM'19) <a href="http://ivg.au.tsinghua.edu.cn/him19/"> [website]</a>
                                        </li>
                                        <li style="margin: 5px;">
                                            <b>Conference Reviewer / Program Committee Member:</b> CVPR, ICCV, ICML, NeurIPS, ICLR and so on.
                                        </li>
                                        <li style="margin: 5px;">
                                            <b>Journal Reviewer:</b> TIP, TMM, TCSVT and so on.
                                        </li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </section> -->
        <section class="section services" data-section="section5">
            <div class="container">
                <div class="section-heading">
                    <h2>Academic Activities</h2>
                    <div class="line-dec"></div>
                    <h3>Competitions</h3>
                    <div class="left">
                        <p>
                            <li style="margin: 5px;"> 2nd place in Semi-Supervised Recognition Challenge at FGVC7 (CVPR 2020)</li>
                        </p>
                    </div>
                    <h3>Academic Services</h3>
                    <div class="left">
                        <p>
                            <li style="margin: 5px;">
                                <b>Publicity Chairs:</b> for CLeaR 2023.
                            </li>
                            <li style="margin: 5px;">
                                <b>Co-organizer:</b> for the ICME 2019 workshop: The Third Workshop on Human Identification in Multimedia (HIM'19) <a href="http://ivg.au.tsinghua.edu.cn/him19/"> [website]</a>
                            </li>
                            <li style="margin: 5px;">
                                <b>Conference Reviewer / Program Committee Member:</b> CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR and so on.
                            </li>
                            <li style="margin: 5px;">
                                <b>Journal Reviewer:</b> TIP, IJCV, TMM, TCSVT and so on.
                            </li>
                        </p>
                    </div>
                </div>
            </div>
        </section>
    </div>
    <!-- Scripts -->
    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="assets/js/isotope.min.js"></script>
    <script src="assets/js/owl-carousel.js"></script>
    <script src="assets/js/lightbox.js"></script>
    <script src="assets/js/custom.js"></script>
    <script>
    //according to loftblog tut
    $(".main-menu li:first").addClass("active");

    var showSection = function showSection(section, isAnimate) {
        var direction = section.replace(/#/, ""),
            reqSection = $(".section").filter(
                '[data-section="' + direction + '"]'
            ),
            reqSectionPos = reqSection.offset().top - 0;

        if (isAnimate) {
            $("body, html").animate({
                    scrollTop: reqSectionPos
                },
                800
            );
        } else {
            $("body, html").scrollTop(reqSectionPos);
        }
    };

    var checkSection = function checkSection() {
        $(".section").each(function() {
            var $this = $(this),
                topEdge = $this.offset().top - 80,
                bottomEdge = topEdge + $this.height(),
                wScroll = $(window).scrollTop();
            if (topEdge < wScroll && bottomEdge > wScroll) {
                var currentId = $this.data("section"),
                    reqLink = $("a").filter("[href*=\\#" + currentId + "]");
                reqLink
                    .closest("li")
                    .addClass("active")
                    .siblings()
                    .removeClass("active");
            }
        });
    };

    $(".main-menu").on("click", "a", function(e) {
        e.preventDefault();
        showSection($(this).attr("href"), true);
    });

    $(window).scroll(function() {
        checkSection();
    });
    </script>
</body>

</html>