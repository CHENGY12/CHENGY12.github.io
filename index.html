<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Guangyi Chen</title>
  
  <meta name="author" content="Guangyi Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Guangyi Chen</name>
              </p>
              <p> 
                I am a fifth year Ph.D student in the Department of Automation at Tsinghua University, advised by Prof. <a href="http://www.au.tsinghua.edu.cn/info/1110/1583.htm"> Jie Zhou </a> and Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>. In 2016, I obtained my B.Eng. in the Department of Automation, Tsinghua University.
              </p>
              <p>
              I am broadly interested in computer vision and deep learning. My current research focuses on attention learning, causal reasoning, video understanding, and person re-identification.
              </p>
              <p style="text-align:center">
                <a href="mailto:chen-gy16@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="files/Resume_GuangyiChen.pdf">CV</a> &nbsp/&nbsp
                <a href="files/Research_Statement.pdf">Research	Statement</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=sAn2eyQAAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/CHENGY12"> Github </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/Guangyi_Chen1.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2021-7:</b> 1 paper on person re-identification and attention learning is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP'2021</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-7:</b> 3 paper on trajectory prediction and attention learning is accepted by <a href="http://iccv2021.thecvf.com/">ICCV'2021</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-3:</b> 1 paper on unintentional action localization is accepted by <a href="https://2021.ieeeicme.org/">ICME'2021</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2020-7:</b> 2 papers on person re-identification are accepted by <a href="https://eccv2020.eu/">ECCV'2020</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2020-06:</b> Our team pangpang (I and <a href="https://raoyongming.github.io/">Yongming</a>) won the 2nd place in Semi-Supervised Recognition Challenge at <a href="https://sites.google.com/view/fgvc7">FGVC7</a> (CVPR 2020).
              </li>
              <li style="margin: 5px;" >
                <b>2020-05:</b> 1 paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
              </li>
              <li style="margin: 5px;">
                <b>2019-07:</b> 2 papers are accepted by <a href="http://iccv2019.thecvf.com/">ICCV'2019</a>.
              </li>
              <li style="margin: 5px;">
                <b>2019-03:</b> 1 paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/apnet.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Person Re-identification via Attention Pyramid</papertitle>
              <br>
              <strong>Guangyi Chen</strong>, Tianpei Gu, Jiwen Lu, Jin-An Bao, and Jie Zhou
              <br>
              <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2021
              <br>
              <a href="https://chengy12.github.io/files/TIP-24326-2021R2.pdf">[PDF]</a> <a href="https://chengy12.github.io/files/TIP-24326-2021supp.pdf">[Supp]</a> <a href="https://github.com/CHENGY12/APNet">[Code]</a> 
              <br>
              <p></p>
              <p>We propose attention pyramid networks by the "split-attend-merge-stack" principle to jointly learn the attentions under different scales and obtain superior performance on many person re-identification datasets.
              </p>
            </td>
          </tr>


            <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/icme21.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Temporal Label Aggregation for Unintentional Action Localization</papertitle>
              <br>
              Nuoxing Zhou, <strong>Guangyi Chen</strong>, Jinglin Xu, Weishi Zheng, and Jiwen Lu
              <br>
              <em>2021 IEEE International Conference on Multimedia and Expo  (<strong>ICME</strong>)</em>, 2021
              <br>
              <a href="https://chengy12.github.io/files/icme2021_final.pdf">[PDF]</a>
              <br>
              <p></p>
              <p>We formulate the unintentional action localization as a temporal probabilistic regression problem, and propose to online aggregate multiple annotations using an attention model.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/0648.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?</papertitle>
              <br>
              <strong>Guangyi Chen*</strong>, Yongming Rao*, Jiwen Lu and Jie Zhou
              <br>
              <em>Proceedings of the European Conference on Computer Vision  (<strong>ECCV</strong>)</em>, 2020
              <br>
              <a href="https://chengy12.github.io/files/0648.pdf">[PDF]</a>
              <br>
              <p></p>
              <p>We show temporal coherence plays a more critical role than temporal motion for video-based person ReID and develop an adversarial feature augmentation to highlight temporal coherence.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/framework_0645.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Credible Metric Learning for Unsupervised Domain Adaptation Person Re-identification</papertitle>
              <br>
              <strong>Guangyi Chen</strong>, Yuhao Lu, Jiwen Lu and Jie Zhou
              <br>
              <em>16th European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
              <br>
              <a href="https://chengy12.github.io/files/0645.pdf">[PDF]</a>
              <br>
              <p></p>
              <p>We propose to adaptively and progressively mine credible training samples to avoid the damage from the noise of predicted pseudo labels for unsupervised domain adaptation person ReID.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/motivation_1040.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Meta Metric Learning</papertitle>
              <br>
              <strong>Guangyi Chen</strong>, Tianren Zhang, Jiwen Lu and Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019
              <br>
              <a href="https://chengy12.github.io/files/1040_camera_ready_final.pdf">[PDF]</a> <a href="https://github.com/CHENGY12/DMML">[Code]</a> 
              <br>
              <p></p>
              <p>We propose to understand the deep metric learning via meta-learning.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/1046.PNG" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Self-Critical Attention Learning for Person Re-Identification</papertitle>
              <br>
              <strong>Guangyi Chen</strong>, Chunze Lin, Liangliang Ren, Jiwen Lu and Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2019
              <br>
              <a href="https://chengy12.github.io/files/1046_camera_ready_final.pdf">[PDF]</a> 
              <br>
              <p></p>
              <p>We present a self-critical attention learning method which applies a critic module to examine and surpervise the attention model.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/tip2020_network.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Recurrent 3D Attention for Video-Based Person Re-identification</papertitle>
              <br>
              <strong>Guangyi Chen</strong>, Jiwen Lu, Ming Yang, and Jie Zhou
              <br>
              <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2020
              <br>
              <a href="https://chengy12.github.io/files/TIP-21136-2019R1.pdf">[PDF]</a> 
              <br>
              <p> We propose to recurrently discover the 3D attention regions and use the reinforcement learning for optimization. </p>
            </td>
          </tr>
	  
	  
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/tip19.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Spatial-Temporal Attention-aware Learning for Video-based Person Re-identification</papertitle>
              <br>
              <strong>Guangyi Chen</strong>, Jiwen Lu, Ming Yang, and Jie Zhou
              <br>
              <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2019
              <br>
              <a href="https://chengy12.github.io/files/TIP-19329-2018.R2.pdf">[PDF]</a> 
              <br>
              <p> We propose a spatial-temporal attention to jointly discover the salient clues in both spatial and temporal domain. </p>
            </td>
          </tr>
	
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ICIP17.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Localized multi-kernel discriminative canonical correlation analysis for video-based person re-identification</papertitle>
              <br>
             <strong>Guangyi Chen</strong>, Jiwen Lu, Jianjiang Feng, and Jie Zhou
              <br>
              <em>IEEE International Conference on Image Processing (<strong>ICIP</strong>)</em>, 2017
              <br>
              <a href="https://chengy12.github.io/files/Localized multi-kernel discriminative canonical correlation analysis for video-based person re-identification.pdf">[PDF]</a> 
              <br>
              <p> We model each pedestrian video as a point on the Riemannian manifold and learn similarity over these points under the multiple kernel learning framework. </p>
            </td>
          </tr>

          

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Competition Awards</heading>
              <p>
<!--                 <li style="margin: 5px;"> ECCV 2020 Outstanding Reviewer</li> -->
                <li style="margin: 5px;"> 2nd place in Semi-Supervised Recognition Challenge at FGVC7 (CVPR 2020)</li>
<!--                 <li style="margin: 5px;"> 2019 CCF-CV Academic Emerging Award (CCF-CV 学术新锐奖)</li>
                <li style="margin: 5px;"> 2019 National Scholarship, Tsinghua University </li>
                <li style="margin: 5px;"> ICME 2019 Best Reviewer Award </li>
                <li style="margin: 5px;"> 2017 Sensetime Undergraduate Scholarship </li>
                <li style="margin: 5px;"> 1st place in 17th Electronic Design Contest of Tsinghua University </li>
                <li style="margin: 5px;"> 1st place in Momenta Lane Detection Challenge </li> -->
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Co-organizer:</b> for the ICME 2019 workshop: The Third Workshop on Human Identification in Multimedia (HIM'19) <a href="http://ivg.au.tsinghua.edu.cn/him19/"> [website]</a>
              </li>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / Program Committee Member:</b> CVPR, ICML, ICCV, NeurIPS and so on.
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  TIP, TMM, TCSVT and so on.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
	  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=yp3s8rdiQW_pbzmBOzWDx2Fv6afIlEpV-k1EZiYIkEY"></script>
	  </div>        
	  <br>
	    &copy; Guangyi Chen | Last updated: Feb 20, 2021
</center></p>
</body>

</html>
